# Pre-Class Practice Notes

My Autonomous Orchestra Performer (AOP) this week uses Processing js, Sonic PI, NexusUI, OSC, and Drum RNN to make a generated soundscape is controlled by the sound emitted by the environment.

Currently there are problems with feedback as expected, hence I will need a cable for line in, and have processing calculate the amp from there.

For now, I will be testing the system using headphones. 

My goal this class, is to make sure my system will blend into the soundscape of the class. I will be participating but as a silent member of the orchestra until I can manage the tweaks. Some tweaks I already know I want:

 - I need a quit mute in Processing, a visualisation that it is sending, a print out the amp value being send in processing.
 
 - In NexusUI, I need a quicker way to start up the sending, a single button that selects a kit and generates a pattern and sends both the original and generated pattern along with the kit selected - a quick boot after making a pattern. I also want a quick generate button for the original pattern.
 
- In Sonic Pi, I want to reduce the log outputs.

Other things to do
- [ ] Fix the documentation of the project, including the diagram of the various parts and how they communicate with each other.
- [ ] After this first class run, write a review of how my performer worked.
	- [ ] Did the sounds blend, which sounds in the soundscape kit should I use?
	- [ ] Do I need to adjust the volumes?
	- [ ] Can it pick up the sound well?
	- [ ] Is it overbearing?
	- [ ] What other tweaks are needed?
	- [ ] How can I fix the UI (Both Processing and the NexasUI) to make start up and control easier to manage
	- [ ] What other controls to I need? 

---

# Post-Class Practice Notes